{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from afutil import get_patch_metadata, read_or_calc_focal_planes, compile_deterministic_data,\\\n",
    "    feature_vector_generator_fn, plot_results, MagellanWithAnnotation\n",
    "from defocusnetwork import DefocusNetwork\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of whats happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWrapper:\n",
    "    \"\"\"\n",
    "    This class wraps all the essential functionality needed for training single shot autofocus. By default, it reads\n",
    "    Micro-Magellan datasets (with an added subclass to be able to store the results of intermediate computations in \n",
    "    an HDF5 file), but this could be replaced with any data source so long as it provides the following\n",
    "    methods and fields\n",
    "    \"\"\"\n",
    "    def __init__(self, magellan):\n",
    "        self.magellan = magellan\n",
    "\n",
    "    #TODO: remember to change to float\n",
    "    def read_ground_truth_image(self, position_index, z_index):\n",
    "        \"\"\"\n",
    "        Read image in which focus quality can be measured form quality of image\n",
    "        :param pos_index: index of xy position\n",
    "        :param z_index: index of z slice (starting at 0)\n",
    "        :param xy_slice: (cropped region of image)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.magellan.read_image(channel_name='DPC_Bottom', pos_index=position_index, \n",
    "                                        z_index=z_index).astype(np.float)\n",
    "\n",
    "    def read_prediction_image(self, position_index, z_index, patch_index, split_k):\n",
    "        \"\"\"\n",
    "        Read image used for single shot prediction (i.e. single LED image)\n",
    "        :param pos_index: index of xy position\n",
    "        :param z_index: index of z slice (starting at 0)\n",
    "        :param split_k: number of crops along each dimension\n",
    "        :param patch_index: index of the crop\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        patch_size, patches_per_image = get_patch_metadata((self.get_image_width(),\n",
    "                                                            self.get_image_height()), split_k)\n",
    "        y_tile_index = patch_index // split_k\n",
    "        x_tile_index = patch_index % split_k\n",
    "        xy_slice = [[y_tile_index * patch_size, (y_tile_index + 1) * patch_size],\n",
    "                    [x_tile_index * patch_size, (x_tile_index + 1) * patch_size]]\n",
    "        return self.magellan.read_image(channel_name='autofocus', pos_index=position_index,\n",
    "                                   z_index=z_index)[xy_slice[0][0]:xy_slice[0][1]][xy_slice[1][0]:xy_slice[1][1]]\n",
    "        \n",
    "    def get_image_width(self):\n",
    "        \"\"\"\n",
    "        :return: image width in pixels\n",
    "        \"\"\"\n",
    "        return self.magellan.image_width\n",
    "\n",
    "    def get_image_height(self):\n",
    "        \"\"\"\n",
    "        :return: image height in pixels\n",
    "        \"\"\"\n",
    "        return self.magellan.image_height\n",
    "\n",
    "    def get_num_z_slices_at(self, position_index):\n",
    "        \"\"\"\n",
    "        return number of z slices (i.e. focal planes) at the given XY position\n",
    "        :param position_index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.magellan.get_num_z_slices_at(position_index)\n",
    "\n",
    "    def get_pixel_size_z_um(self):\n",
    "        \"\"\"\n",
    "        :return: distance in um between consecutive z slices\n",
    "        \"\"\"\n",
    "        return self.magellan.pixel_size_z_um\n",
    "\n",
    "    def get_num_xy_positions(self):\n",
    "        \"\"\"\n",
    "        :return: total number of xy positons in data set\n",
    "        \"\"\"\n",
    "        return self.magellan.get_num_xy_positions()\n",
    "\n",
    "    def store_focal_plane(self, name, focal_position):\n",
    "        \"\"\"\n",
    "        Store the computed focal plane as a string, float pair\n",
    "        \"\"\"\n",
    "        self.magellan.write_annotation(name, focal_position)\n",
    "\n",
    "    def read_focal_plane(self, name):\n",
    "        \"\"\"\n",
    "        read a previously computed focal plane\n",
    "        :param name: key corresponding to an xy position for whch focal plane has already been computed\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.magellan.read_annotation(name)\n",
    "\n",
    "    def store_array(self, name, array):\n",
    "        \"\"\"\n",
    "        Store a numpy array containing the design matrix for training the non-deterministic part of the network (i.e.\n",
    "        after the Fourier transform) so that it can be retrained quickly without having to recompute\n",
    "        :param name:\n",
    "        :param array: (n examples) x (d feature length) numpy array\n",
    "        \"\"\"\n",
    "        self.magellan.store_array(name, array)\n",
    "\n",
    "    def read_array(self, name):\n",
    "        \"\"\"\n",
    "        Read and return a previously computed array\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.magellan.read_array(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load data and compute the ground truth focal planes as targets for training. The show_output flag will create a plot of the averaged high frequency content of the log power spectrum. The maximum of the this plot should correspond to the correct focal plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\nReading precomputed focal plane\n"
     ]
    }
   ],
   "source": [
    "#parameters for the deterministic part of the network\n",
    "#TODO: better explain what these mean\n",
    "deterministic_params = {'non_led_width': 0.1, 'led_width': 0.6, 'tile_split_k': 2}\n",
    "\n",
    "#load data\n",
    "data = [DataWrapper(MagellanWithAnnotation(\n",
    "    '/home/henry/data/2018-9-27 Cells and histology af data/unstained path section 12x12 30um range 1um step_1'))]\n",
    "\n",
    "#load or compute target focal planes using 22 CPU cores to speed computation\n",
    "train_focal_planes = {dataset: read_or_calc_focal_planes(dataset, split_k=deterministic_params['tile_split_k'],\n",
    "                                                         n_cores=6, show_output=True) for dataset in data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEDs in vertical axis of array:\n",
    "4 12 28 48 83 119 187\n",
    "3 11 27 47 84 120 188\n",
    "3 has defect in it, 4 doesnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2961 sliceposition tuples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trainable graph...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DefocusNetwork' object has no attribute 'hyperparams'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-c157d7ed7e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m defocus_prediction_network = DefocusNetwork(input_shape=features.shape[1], train_generator=train_generator,\n\u001b[1;32m     16\u001b[0m                              \u001b[0mval_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_input_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                              deterministic_params=deterministic_params, regressor_only=True, train_mode='train')\n\u001b[0m",
      "\u001b[0;32m~/Google Drive/Code/GitRepos/DeepAF/defocusnetwork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, train_generator, regressor_only, deterministic_params, val_generator, predict_input_shape, train_mode)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_normalizations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mpredict_input_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_output_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_input_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_input_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_output_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_output_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/Code/GitRepos/DeepAF/defocusnetwork.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, load_variables)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# build seperate graphs because they each have different input Datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mvalidation_error_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error_init_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mpredict_input_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_output_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/Code/GitRepos/DeepAF/defocusnetwork.py\u001b[0m in \u001b[0;36m_build_graph\u001b[0;34m(self, graph_mode, dataset)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mnormalized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;31m#a feature vector as input being fed into one or more hidden layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mnormalized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_dropout_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0;31m# other layers (containing weights, biases as tf.Variables)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# regularizer = tf.contrib.layers.l1_regularizer(scale=self.hyper_params['regularization_strength'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DefocusNetwork' object has no attribute 'hyperparams'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#compute or load already computed design matrices\n",
    "features, targets = compile_deterministic_data(data, train_focal_planes, deterministic_params=deterministic_params)\n",
    "\n",
    "#make genenrator function for providing training examples and seperate validation generator for assessing its progress\n",
    "train_generator = feature_vector_generator_fn(features, targets, mode='training', \n",
    "                                        split_k=deterministic_params['tile_split_k'], training_fraction=0.8)\n",
    "val_generator = feature_vector_generator_fn(features, targets, mode='validation', \n",
    "                                        split_k=deterministic_params['tile_split_k'], training_fraction=0.8)\n",
    "\n",
    "#feed in the dimensions of the cropped input so the inference network knows what to expect\n",
    "patch_size, patches_per_image = get_patch_metadata((data[0].get_image_width(),\n",
    "                                        data[0].get_image_height()), deterministic_params['tile_split_k'])\n",
    "\n",
    "#this call creates network and trains it\n",
    "defocus_prediction_network = DefocusNetwork(input_shape=features.shape[1], train_generator=train_generator,\n",
    "                             val_generator=val_generator, predict_input_shape=[patch_size, patch_size],\n",
    "                             deterministic_params=deterministic_params, regressor_only=True, train_mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze performance and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defocus_prediction_network' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f414724013c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#visualize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#validation data drawn from same set used in training, test data is a seperate file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_prediction_defocus, train_target_defocus = defocus_prediction_network.analyze_performance(\n\u001b[0m\u001b[1;32m      4\u001b[0m     feature_vector_generator_fn(train_features, train_targets, mode='all', split_k=deterministic_params['tile_split_k']))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defocus_prediction_network' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#visualize results\n",
    "#validation data drawn from same set used in training, test data is a seperate file\n",
    "train_prediction_defocus, train_target_defocus = defocus_prediction_network.analyze_performance(\n",
    "    feature_vector_generator_fn(train_features, train_targets, mode='all', split_k=deterministic_params['tile_split_k']))\n",
    "\n",
    "test_prediction_defocus, test_target_defocus = defocus_prediction_network.analyze_performance(\n",
    "        feature_vector_generator_fn(test_features, test_targets, mode='all', split_k=deterministic_params['tile_split_k']))\n",
    "\n",
    "#average predictions\n",
    "train_pred_avg, train_target_avg = average_predictions(train_prediction_defocus, train_target_defocus, deterministic_params['tile_split_k'] ** 2)\n",
    "test_pred_avg, test_target_avg = average_predictions(test_prediction_defocus, test_target_defocus, deterministic_params['tile_split_k']**2)\n",
    "\n",
    "plt.figure(1)\n",
    "plot_results(train_prediction_defocus, train_target_defocus, 'Training')\n",
    "plot_results(test_prediction_defocus, test_target_defocus, 'Test', draw_rect=True)\n",
    "plt.legend(['Training data average', 'Test data averaged', 'Ground truth', 'Objective depth of focus'])\n",
    "\n",
    "#show data before tile averaging\n",
    "plt.figure(2)\n",
    "plot_results(train_pred_avg, train_target_avg, 'Training')\n",
    "plot_results(test_pred_avg, test_target_avg, 'Test', draw_rect=True)\n",
    "plt.legend(['Training data average', 'Test data averaged', 'Ground truth', 'Objective depth of focus'])\n",
    "\n",
    "plt.show(block=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
