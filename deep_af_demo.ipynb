{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-shot autofocus using deep learning\n",
    "This notebook provides all code neccessary for and shows the production of all figures for the single-shot autofocus using deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from afutil import get_patch_metadata, read_or_calc_focal_planes, compile_deterministic_data,\\\n",
    "    feature_vector_generator_fn, MagellanWithAnnotation\n",
    "from defocusnetwork import DefocusNetwork\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataWrapper class wraps all the essential functionality needed for training single shot autofocus. By default, it reads Micro-Magellan datasets (with an added subclass to be able to store the results of intermediate computations in an HDF5 file), but this could be replaced with any data source so long as it provides the following methods and fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWrapper:\n",
    "\n",
    "    def __init__(self, magellan):\n",
    "        self.magellan = magellan\n",
    "\n",
    "    #TODO: remember to change to float\n",
    "    def read_ground_truth_image(self, position_index, z_index):\n",
    "        \"\"\"\n",
    "        Read image in which focus quality can be measured form quality of image\n",
    "        :param pos_index: index of xy position\n",
    "        :param z_index: index of z slice (starting at 0)\n",
    "        :param xy_slice: (cropped region of image)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.magellan.read_image(channel_name='DPC_Bottom', pos_index=position_index, \n",
    "                        z_index=z_index + min(self.magellan.get_z_slices_at(position_index))).astype(np.float)\n",
    "\n",
    "    def read_prediction_image(self, position_index, z_index, patch_index, split_k):\n",
    "        \"\"\"\n",
    "        Read image used for single shot prediction (i.e. single LED image)\n",
    "        :param pos_index: index of xy position\n",
    "        :param z_index: index of z slice (starting at 0)\n",
    "        :param split_k: number of crops along each dimension\n",
    "        :param patch_index: index of the crop\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        patch_size, patches_per_image = get_patch_metadata((self.get_image_width(),\n",
    "                                                            self.get_image_height()), split_k)\n",
    "        y_tile_index = patch_index // split_k\n",
    "        x_tile_index = patch_index % split_k\n",
    "        xy_slice = [[y_tile_index * patch_size, (y_tile_index + 1) * patch_size],\n",
    "                    [x_tile_index * patch_size, (x_tile_index + 1) * patch_size]]\n",
    "        image = self.magellan.read_image(channel_name='autofocus', pos_index=position_index, z_index=z_index +\n",
    "                  min(self.magellan.get_z_slices_at(position_index))).astype(np.float)\n",
    "        #crop\n",
    "        return image[xy_slice[0][0]:xy_slice[0][1], xy_slice[1][0]:xy_slice[1][1]]\n",
    "        \n",
    "    def get_image_width(self):\n",
    "        \"\"\"\n",
    "        :return: image width in pixels\n",
    "        \"\"\"\n",
    "        return self.magellan.image_width\n",
    "\n",
    "    def get_image_height(self):\n",
    "        \"\"\"\n",
    "        :return: image height in pixels\n",
    "        \"\"\"\n",
    "        return self.magellan.image_height\n",
    "\n",
    "    def get_num_z_slices_at(self, position_index):\n",
    "        \"\"\"\n",
    "        return number of z slices (i.e. focal planes) at the given XY position\n",
    "        :param position_index:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return len(self.magellan.get_z_slices_at(position_index))\n",
    "\n",
    "    def get_pixel_size_z_um(self):\n",
    "        \"\"\"\n",
    "        :return: distance in um between consecutive z slices\n",
    "        \"\"\"\n",
    "        return self.magellan.pixel_size_z_um\n",
    "\n",
    "    def get_num_xy_positions(self):\n",
    "        \"\"\"\n",
    "        :return: total number of xy positons in data set\n",
    "        \"\"\"\n",
    "        return self.magellan.get_num_xy_positions()\n",
    "\n",
    "    def store_focal_plane(self, name, focal_position):\n",
    "        \"\"\"\n",
    "        Store the computed focal plane as a string, float pair\n",
    "        \"\"\"\n",
    "        self.magellan.write_annotation(name, focal_position)\n",
    "\n",
    "    def read_focal_plane(self, name):\n",
    "        \"\"\"\n",
    "        read a previously computed focal plane\n",
    "        :param name: key corresponding to an xy position for whch focal plane has already been computed\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.magellan.read_annotation(name)\n",
    "\n",
    "    def store_array(self, name, array):\n",
    "        \"\"\"\n",
    "        Store a numpy array containing the design matrix for training the non-deterministic part of the network (i.e.\n",
    "        after the Fourier transform) so that it can be retrained quickly without having to recompute\n",
    "        :param name:\n",
    "        :param array: (n examples) x (d feature length) numpy array\n",
    "        \"\"\"\n",
    "        self.magellan.store_array(name, array)\n",
    "\n",
    "    def read_array(self, name):\n",
    "        \"\"\"\n",
    "        Read and return a previously computed array\n",
    "        :param name:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.magellan.read_array(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and compute the ground truth focal planes as targets for training\n",
    "The show_output flag will create a plot of the averaged high frequency content of the log power spectrum. If new data is substituted in, the maximum of the this plot should correspond to the correct focal plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-28a673c762c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     '/media/cosmosdata/henry/data/2018-9-27 Cells and histology af data/Neomounted cells 12x12 30um range 1um step_1'))\n\u001b[1;32m      8\u001b[0m histology_data = DataWrapper(MagellanWithAnnotation(\n\u001b[0;32m----> 9\u001b[0;31m     '/media/cosmosdata/henry/data/2018-9-27 Cells and histology af data/unstained path section 12x12 30um range 1um step_1'))\n\u001b[0m\u001b[1;32m     10\u001b[0m multi_illumination_data = DataWrapper(MagellanWithAnnotation(\n\u001b[1;32m     11\u001b[0m     '/media/cosmosdata/henry/data/2018-9-26 Single LED autofocus data/Orthogonal lines LEDs 30um range 1um step_1'))\n",
      "\u001b[0;32m~/GitRepos/DeepAF/afutil.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_path)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitRepos/DeepAF/pygellan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_path)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mres_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres_dirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mres_dir_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mres_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMagellanResolutionLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_dir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mres_dir\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Full resolution'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_levels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitRepos/DeepAF/pygellan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m#populate list of readers and tree mapping indices to readers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtiff\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtiff_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMagellanMultipageTiffReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitRepos/DeepAF/pygellan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tiff_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# memory map the entire file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_ifd_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;31m# get important metadata fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitRepos/DeepAF/pygellan.py\u001b[0m in \u001b[0;36m_read_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \"\"\"\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# read standard tiff header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'\\x4d\\x4d'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;31m# Big endian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyteorder\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'big'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#parameters for the deterministic part of the network\n",
    "#TODO: better explain what these mean\n",
    "deterministic_params = {'non_led_width': 0.1, 'led_width': 0.6, 'tile_split_k': 2}\n",
    "\n",
    "#load data\n",
    "cell_data = DataWrapper(MagellanWithAnnotation(\n",
    "    '/media/cosmosdata/henry/data/2018-9-27 Cells and histology af data/Neomounted cells 12x12 30um range 1um step_1'))\n",
    "histology_data = DataWrapper(MagellanWithAnnotation(\n",
    "    '/media/cosmosdata/henry/data/2018-9-27 Cells and histology af data/unstained path section 12x12 30um range 1um step_1'))\n",
    "multi_illumination_data = DataWrapper(MagellanWithAnnotation(\n",
    "    '/media/cosmosdata/henry/data/2018-9-26 Single LED autofocus data/Orthogonal lines LEDs 30um range 1um step_1'))\n",
    "\n",
    "#load or compute target focal planes using 22 CPU cores to speed computation\n",
    "focal_planes = {dataset: read_or_calc_focal_planes(dataset, split_k=deterministic_params['tile_split_k'],\n",
    "                    n_cores=22, show_output=True) for dataset in [cell_data, histology_data, multi_illumination_data]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network using cell data, compute validation on other cell data and histology section data\n",
    "The deterministic (i.e. Fourier Transform) beginning part of the network is precomputed and stored, so that it doesn't have to be recomputed on each training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7892 sliceposition tuples\n",
      "919 sliceposition tuples\n",
      "896 sliceposition tuples\n",
      "Building trainable graph...\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable hidden0/kernel/Adam/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/home/henry/GitRepos/DeepAF/defocusnetwork.py\", line 389, in _build_graph\n    train_step = tf.train.AdamOptimizer(self.hyper_params['learning_rate']).minimize(total_loss, global_step=tf.train.get_global_step())\n  File \"/home/henry/GitRepos/DeepAF/defocusnetwork.py\", line 106, in _train\n    train_op = self._build_graph(graph_mode='training', dataset=train_dataset)\n  File \"/home/henry/GitRepos/DeepAF/defocusnetwork.py\", line 44, in __init__\n    predict_input_op, predict_output_op = self._train()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5f92e0eb9603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m defocus_prediction_network = DefocusNetwork(input_shape=train_features.shape[1], train_generator=train_generator,\n\u001b[1;32m     35\u001b[0m                              \u001b[0mval_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_input_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                              deterministic_params=deterministic_params, regressor_only=True, train_mode='train')\n\u001b[0m",
      "\u001b[0;32m~/GitRepos/DeepAF/defocusnetwork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, train_generator, regressor_only, deterministic_params, val_generator, predict_input_shape, train_mode)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_normalizations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mpredict_input_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_output_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_input_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_input_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_output_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_output_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitRepos/DeepAF/defocusnetwork.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, load_variables)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# build seperate graphs because they each have different input Datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mvalidation_error_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error_init_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'evaluate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mpredict_input_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_output_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitRepos/DeepAF/defocusnetwork.py\u001b[0m in \u001b[0;36m_build_graph\u001b[0;34m(self, graph_mode, dataset)\u001b[0m\n\u001b[1;32m    387\u001b[0m                         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_reg_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                     \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m                     \u001b[0;31m# train_step = tf.train.GradientDescentOptimizer(self.hyper_params['learning_rate']).minimize(loss, global_step=tf.train.get_global_step())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[0;32m--> 410\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    586\u001b[0m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/adam.py\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Create slots for the first and second moments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_zeros_slot\u001b[0;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       \u001b[0mnew_slot_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m       self._restore_slot_variable(\n\u001b[1;32m   1136\u001b[0m           \u001b[0mslot_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[0;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[1;32m    179\u001b[0m     return create_slot_with_initializer(\n\u001b[1;32m    180\u001b[0m         \u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslot_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         colocate_with_primary=colocate_with_primary)\n\u001b[0m\u001b[1;32m    182\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot_with_initializer\u001b[0;34m(primary, initializer, shape, dtype, name, colocate_with_primary)\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_vars_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n\u001b[0;32m--> 155\u001b[0;31m                                 dtype)\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m       return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36m_create_slot_var\u001b[0;34m(primary, val, scope, validate_shape, shape, dtype)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_resource_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m     66\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1465\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1467\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1215\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    525\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    479\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    846\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 848\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable hidden0/kernel/Adam/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/home/henry/GitRepos/DeepAF/defocusnetwork.py\", line 389, in _build_graph\n    train_step = tf.train.AdamOptimizer(self.hyper_params['learning_rate']).minimize(total_loss, global_step=tf.train.get_global_step())\n  File \"/home/henry/GitRepos/DeepAF/defocusnetwork.py\", line 106, in _train\n    train_op = self._build_graph(graph_mode='training', dataset=train_dataset)\n  File \"/home/henry/GitRepos/DeepAF/defocusnetwork.py\", line 44, in __init__\n    predict_input_op, predict_output_op = self._train()\n"
     ]
    }
   ],
   "source": [
    "#split cell data into training and validation sets\n",
    "num_pos = cell_data.get_num_xy_positions()\n",
    "train_positions = list(range(int(num_pos * 0.9)))\n",
    "validation_positions = list(range(max(train_positions) + 1, num_pos))\n",
    "#use subset of histology dataset to save comput time\n",
    "validation_positions_histology = list(range(len(validation_positions)))\n",
    "\n",
    "#Compute or load already computed design matrices\n",
    "train_features, train_targets = compile_deterministic_data([cell_data], [train_positions], focal_planes,\n",
    "                                                           deterministic_params=deterministic_params)\n",
    "validation_features_cells, validation_targets_cells = compile_deterministic_data([cell_data], [validation_positions],\n",
    "                                                    focal_planes, deterministic_params=deterministic_params)\n",
    "validation_features_histology, validation_targets_histolgoy = compile_deterministic_data([histology_data], \n",
    "                        [validation_positions_histology], focal_planes, deterministic_params=deterministic_params)\n",
    "\n",
    "#TODO: split cell here\n",
    "\n",
    "#make genenrator function for providing training examples and seperate validation generator for assessing its progress\n",
    "#stop training once error on validation set stops decreasing\n",
    "train_generator = feature_vector_generator_fn(train_features, train_targets, mode='all', \n",
    "                                        split_k=deterministic_params['tile_split_k'])\n",
    "val_generator = feature_vector_generator_fn(validation_features_cells, validation_targets_cells, mode='all', \n",
    "                                        split_k=deterministic_params['tile_split_k'])\n",
    "val_generator_histology = feature_vector_generator_fn(validation_features_cells, validation_targets_cells, mode='all', \n",
    "                                        split_k=deterministic_params['tile_split_k'])\n",
    "\n",
    "#feed in the dimensions of the cropped input so the inference network knows what to expect\n",
    "#although the inference network is not explicitly used in this notebook, it is created so that the model tensforflow\n",
    "#creates could later be used on real data\n",
    "patch_size, patches_per_image = get_patch_metadata((cell_data.get_image_width(),\n",
    "                                        cell_data.get_image_height()), deterministic_params['tile_split_k'])\n",
    "\n",
    "#Create network and train it\n",
    "defocus_prediction_network = DefocusNetwork(input_shape=train_features.shape[1], train_generator=train_generator,\n",
    "                             val_generator=val_generator, predict_input_shape=[patch_size, patch_size],\n",
    "                             deterministic_params=deterministic_params, regressor_only=True, train_mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network performance on different parts of the same sample and different samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defocus_prediction_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f414724013c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#visualize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#validation data drawn from same set used in training, test data is a seperate file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_prediction_defocus, train_target_defocus = defocus_prediction_network.analyze_performance(\n\u001b[0m\u001b[1;32m      4\u001b[0m     feature_vector_generator_fn(train_features, train_targets, mode='all', split_k=deterministic_params['tile_split_k']))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'defocus_prediction_network' is not defined"
     ]
    }
   ],
   "source": [
    "#run training set and both valdation sets through network to generate predictions\n",
    "train_prediction_defocus, train_target_defocus = defocus_prediction_network.predict_validation(train_generator)\n",
    "val_prediction_defocus_cells, val_target_defocus_cells = defocus_prediction_network.predict_validation(val_generator)\n",
    "test_prediction_defocus_histology, test_target_defocus_histoloy = defocus_prediction_network.predict_validation(val_generator_histology)\n",
    "\n",
    "def average_predictions(pred, target, block_size):\n",
    "    \"\"\"\n",
    "    Take the median of all predictions coming from a single raw image--that is, all crops that came from the same\n",
    "    original image\n",
    "    \"\"\"\n",
    "    pred_consensus = np.median(np.reshape(pred, [-1, block_size]), axis=1)\n",
    "    target_consensus = np.median(np.reshape(target, [-1, block_size]), axis=1)\n",
    "    return pred_consensus, target_consensus\n",
    "\n",
    "\n",
    "train_pred_avg, train_target_avg = average_predictions(train_prediction_defocus, train_target_defocus, \n",
    "                                                       deterministic_params['tile_split_k'] ** 2)\n",
    "val_pred_avg_cells, val_target_avg_cells = average_predictions(val_prediction_defocus_cells, val_target_defocus_cells, \n",
    "                                                     deterministic_params['tile_split_k']**2)\n",
    "val_pred_avg_histology, val_target_avg_histology = average_predictions(test_prediction_defocus_histology, \n",
    "                                                 test_target_defocus_histoloy, deterministic_params['tile_split_k']**2)\n",
    "\n",
    "def plot_results(pred, target, draw_rect=False):\n",
    "    plt.plot(target, pred, '.')\n",
    "    plt.xlabel('Target defocus (um)')\n",
    "    plt.ylabel('Predicted defocus (um)')\n",
    "    if draw_rect:\n",
    "        min_target = np.min(target)\n",
    "        max_target = np.max(target)\n",
    "        height = (max_target - min_target)*np.sqrt(2)\n",
    "        width = 2\n",
    "        plt.gca().add_patch(mpatches.Rectangle([min_target, min_target+width/np.sqrt(2)], width, height,\n",
    "                                               angle=-45, color=[1, 0, 0, 0.2]))\n",
    "        plt.plot([min_target, max_target], [min_target, max_target], 'r-')\n",
    "    print('{} RMSE: {}'.format(name, np.sqrt(np.mean((pred - target) ** 2))))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plot_results(train_prediction_defocus, train_target_defocus, 'Training (cells)')\n",
    "plot_results(test_prediction_defocus, test_target_defocus, draw_rect=True)\n",
    "plot_results(test_prediction_defocus, test_target_defocus, 'Test (histology)', draw_rect=True)\n",
    "plt.legend(['Training set (cells)',  'Test set (cells)', 'Test set (histology section)',\n",
    "            'Ground truth', 'Objective depth of focus'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEDs in vertical axis of array:\n",
    "4 12 28 48 83 119 187\n",
    "3 11 27 47 84 120 188\n",
    "3 has defect in it, 4 doesnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
